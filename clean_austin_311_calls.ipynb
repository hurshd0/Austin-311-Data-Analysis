{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Austin-3-1-1 Data \n",
    "\n",
    "<img src=\"imgs/cleaning.jpg\" width = \"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:10:57.037866Z",
     "start_time": "2019-07-30T09:10:56.415653Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # Linear algebra lib\n",
    "import pandas as pd # Data analysis lib\n",
    "\n",
    "# Removes rows and columns truncation of '...'\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:30.168836Z",
     "start_time": "2019-07-30T09:10:57.042649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-31 01:13:02--  https://austin-311-data.s3.us-east-2.amazonaws.com/311_Unified_Data.csv\n",
      "Resolving austin-311-data.s3.us-east-2.amazonaws.com (austin-311-data.s3.us-east-2.amazonaws.com)... 52.219.96.24\n",
      "Connecting to austin-311-data.s3.us-east-2.amazonaws.com (austin-311-data.s3.us-east-2.amazonaws.com)|52.219.96.24|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 257255577 (245M) [text/csv]\n",
      "Saving to: ‘raw_data/311_Unified_Data.csv’\n",
      "\n",
      "311_Unified_Data.cs 100%[===================>] 245.34M  8.30MB/s    in 30s     \n",
      "\n",
      "2019-07-31 01:13:32 (8.31 MB/s) - ‘raw_data/311_Unified_Data.csv’ saved [257255577/257255577]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Toggle Comments to run\n",
    "!mkdir -p 'raw_data'\n",
    "!rm -f raw_data/'311_Unified_Data.csv'\n",
    "!wget 'https://austin-311-data.s3.us-east-2.amazonaws.com/311_Unified_Data.csv' -P raw_data\n",
    "!ls -lh raw_data\n",
    "!head raw_data/'311_Unified_Data.csv'\n",
    "!tail raw_data/'311_Unified_Data.csv'\n",
    "!wc -l raw_data/'311_Unified_Data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:43.757542Z",
     "start_time": "2019-07-30T09:11:30.180176Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('raw_data/311_Unified_Data.csv', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:43.771222Z",
     "start_time": "2019-07-30T09:11:43.763174Z"
    }
   },
   "outputs": [],
   "source": [
    "print('This dataset has number of rows {}, number of cols {}'.format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:45.801331Z",
     "start_time": "2019-07-30T09:11:43.775010Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis we don't need following columns:\n",
    " \n",
    "  - `Service Request (SR) Number`\n",
    "  - `Status Change Date`\n",
    "  - `SR Status`\n",
    "  - `Last Update Date`\n",
    "  - `Close Date`\n",
    "  - `Map Page`\n",
    "  - `Map Tile`\n",
    "  - `State Plane X Coordinate`\n",
    "  - `State Plane Y Coordinate`\n",
    "  - `Street Number`\n",
    "  - `Street Name`\n",
    "  - `SR Location`\n",
    "  - `Latitude Coordinate`\n",
    "  - `Longitude Coordinate`\n",
    "  - `Council District`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unecessary columns and empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:45.932323Z",
     "start_time": "2019-07-30T09:11:45.804959Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['Council District', 'Map Page', 'Map Tile', 'Service Request (SR) Number', 'Status Change Date', 'Last Update Date', 'Close Date', 'SR Status', 'SR Location', 'Street Number', 'Street Name', 'State Plane X Coordinate', 'State Plane Y Coordinate', 'Latitude Coordinate', 'Longitude Coordinate']\n",
    "df = df.drop(columns, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values and drop rows that are missing important info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:47.529282Z",
     "start_time": "2019-07-30T09:11:45.936157Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are lot missing values, mostly due to empty rows, let's drop those rows now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:48.947065Z",
     "start_time": "2019-07-30T09:11:47.532953Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna(how='all') # how='all' drops rows that have all NaN values, whereas, 'any' will drop any row that has NaN present\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping rows that had all `NaN` values, now we can drop those that having missing valuable information needed to stratify complaints by their location info. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing all location data (i.e. Zip Code and County and City)\n",
    "\n",
    "Let's drop rows that contain all of the missing location info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:49.313186Z",
     "start_time": "2019-07-30T09:11:48.950828Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Before dimensions: ', df.shape)\n",
    "df = df.loc[df[['City', 'Zip Code', 'County']].notnull().values.any(axis=1)]\n",
    "print('After dimensions: ', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:49.937363Z",
     "start_time": "2019-07-30T09:11:49.316983Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After eliminating rows with no location information, we can now dive into missing `Latitude Longitude` coordinate values, as they are important to our analysis.\n",
    "\n",
    "#### Drop Missing Lat., Long. Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:50.193774Z",
     "start_time": "2019-07-30T09:11:49.996217Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Before dimensions: ', df.shape)\n",
    "df = df.loc[df[['(Latitude.Longitude)']].notnull().values.any(axis=1)]\n",
    "print('After dimensions: ', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Missing Zipcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: We can actually reverse geocode location info like, Street Address, City, County, Zip Code, and State using something like [Google Maps Reverse Geocode API](https://developers.google.com/maps/documentation/geocoding/start#reverse), and have that fill in the missing information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:50.374191Z",
     "start_time": "2019-07-30T09:11:50.198122Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Before dimensions: ', df.shape)\n",
    "df = df.loc[df['Zip Code'].notnull().values]\n",
    "print('After dimensions: ', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out rows that match Zipcodes from Austin and Travis County"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use official City of Austin's to figure out Zip Codes under city territory.\n",
    "\n",
    "![](https://i.imgur.com/kdG5bxm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the polygons with zipcodes intersecting under blue-shaded region are under `City of Austin's` jurisdiction, but it's hazy at best, so we will keep all those complaints, under those zipcodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:50.579044Z",
     "start_time": "2019-07-30T09:11:50.378011Z"
    }
   },
   "outputs": [],
   "source": [
    "austin_zipcodes = \"\"\"\n",
    "78701,78702,78703,78704,78705,\n",
    "78721,78722,78723,78724,78725,\n",
    "78726,78727,78728,78729,78730,\n",
    "78731,78732,78733,78734,78735,\n",
    "78736,78737,78738,78739,78741,\n",
    "78742,78744,78745,78746,78747,\n",
    "78748,78749,78750,78751,78752,\n",
    "78753,78754,78756,78757,78758,\n",
    "78759,78610,78617,78653,78660\n",
    "\"\"\".split(',')\n",
    "austin_zipcodes = [x.strip() for x in austin_zipcodes]\n",
    "print('Before dimensions: ', df.shape)\n",
    "df = df.loc[df['Zip Code'].isin(np.array(austin_zipcodes))]\n",
    "print('After dimensions: ', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:50.753688Z",
     "start_time": "2019-07-30T09:11:50.583295Z"
    }
   },
   "outputs": [],
   "source": [
    "df['County'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T01:54:15.840935Z",
     "start_time": "2019-07-30T01:54:15.359601Z"
    }
   },
   "source": [
    "### Clean `city` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strip leading and trailing whitespaces first from entire dataframe\n",
    "\n",
    "##### Before Stripping whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:51.585272Z",
     "start_time": "2019-07-30T09:11:50.758374Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:55.783450Z",
     "start_time": "2019-07-30T09:11:51.589155Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "df.replace('', np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After stripping whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:56.390703Z",
     "start_time": "2019-07-30T09:11:55.787461Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start by fixing casing of `City` and `County` columns to `Title Case`, and fix any misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:57.142429Z",
     "start_time": "2019-07-30T09:11:56.395144Z"
    }
   },
   "outputs": [],
   "source": [
    "df['County'] = df['County'].str.title()\n",
    "df['County'].value_counts(dropna=False).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:57.888869Z",
     "start_time": "2019-07-30T09:11:57.146385Z"
    }
   },
   "outputs": [],
   "source": [
    "df['City'] = df['City'].str.title()\n",
    "df['City'].value_counts(dropna=False).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so surprisingly we have so many typos, and misspellings, which we can fix, but surprisingly there are calls from cities that are miles apart like `Houston`, `Dallas`, etc.., so let's keep those that are only from `City of Austin` in Travis County which is our main focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Austin and it's Extraterritorial Jurisdiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to note is `Austin's 5 ETJ's` (Extraterritorial Jurisdiction) refers to cities, is the legal capability of a municipality to exercise authority beyond the boundaries of its incorporated area. In the US, Texas is one of the states that by law allow cities to claim ETJ to contiguous land beyond their city limits.  Austin’s ETJ currently extends into 4 counties including Williamson, Travis, Hays, and Bastrop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:58.566967Z",
     "start_time": "2019-07-30T09:11:57.893045Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['City'].str.contains('Austin 5 Etj', na=False), 'County'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:11:59.256892Z",
     "start_time": "2019-07-30T09:11:58.570719Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['City'].str.contains('Austin 5 Etj', na=False), 'Zip Code'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix `Austin` related typos and misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:12:00.133749Z",
     "start_time": "2019-07-30T09:11:59.260639Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[(df['City'] != 'Austin 5 Etj') & df['City'].str.startswith('Aus', na=False).values, 'City'] = 'Austin'\n",
    "df['City'].value_counts(dropna=False).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is considerable complaints from surrounding territories that are under City of Austin's jurisdiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:12:00.320569Z",
     "start_time": "2019-07-30T09:12:00.137859Z"
    }
   },
   "outputs": [],
   "source": [
    "df['County'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:12:00.640723Z",
     "start_time": "2019-07-30T09:12:00.324293Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['County', 'City']].groupby(['County', 'City']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are discrepancies in the data as `Austin` is in `Williamson`, `Bastrop`, and `Hays`, while the zipcodes we filtered only pertained to `City of Austin` territories in `Travis` county.\n",
    "\n",
    "As we noted earlier, using Google Maps API to reverse geocode to actual data would need to be done to assert the validity of `Zip Code`, `City` and `County`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check first to see if we have duplicate rows or not,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:12:02.162257Z",
     "start_time": "2019-07-30T09:12:00.644870Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df.duplicated()].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, there are duplicate rows, let's drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:12:05.275598Z",
     "start_time": "2019-07-30T09:12:02.166042Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Before dimensions: ', df.shape)\n",
    "df = df.drop_duplicates()\n",
    "print('After dimensions: ', df.shape)\n",
    "df[df.duplicated()].head() # Check again to see if duplicate rows are dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now drop the `City` and `County` columns, since we are going to use only `Zip Code` and `Latitude Longitude` columns to map our plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop `City`, and `County` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:12:05.539495Z",
     "start_time": "2019-07-30T09:12:05.279251Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(['City', 'County'], axis=1)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows that have no `Created Date`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis to figure when the `Complaint` was noted, we need `Created Date` to be non-null containing column, so let's drop those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:12:05.637207Z",
     "start_time": "2019-07-30T09:12:05.543262Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df['Created Date'].isnull().values].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:12:06.273891Z",
     "start_time": "2019-07-30T09:12:05.641121Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.loc[df['Created Date'].notnull().values]\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reset the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:12:06.847011Z",
     "start_time": "2019-07-30T09:12:06.278014Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:12:15.089946Z",
     "start_time": "2019-07-30T09:12:06.851045Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change Zip Code from float to integer data type\n",
    "df['Zip Code'] = df['Zip Code'].astype(int)\n",
    "\n",
    "# Change Created Date to datetime object, so we can extract date, month, year, and hour\n",
    "df['Created Date'] = pd.to_datetime(df['Created Date'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "# Add month, year columns\n",
    "df['Incident Year'] = df['Created Date'].dt.year\n",
    "df['Incident Month'] = df['Created Date'].dt.month\n",
    "df['Incident Hour'] = df['Created Date'].dt.hour\n",
    "df['Incident Weekday'] = df['Created Date'].dt.weekday_name\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:12:15.519891Z",
     "start_time": "2019-07-30T09:12:15.098195Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'SR Type Code':'Incident Type Code',\n",
    "    'SR Description':'Incident Description',\n",
    "    'Created Date':'Incident Date'\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:12:15.547437Z",
     "start_time": "2019-07-30T09:12:15.523710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verify we have all incidents from all years\n",
    "df['Incident Year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the clean data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:12:36.613230Z",
     "start_time": "2019-07-30T09:12:15.556750Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf 'clean_data/clean_austin_311.csv'\n",
    "df.to_csv('clean_data/clean_austin_311.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Austin 311 EDA)",
   "language": "python",
   "name": "austin_eda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
